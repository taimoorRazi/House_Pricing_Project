{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c2fac57",
   "metadata": {},
   "source": [
    "# Machine Learning Pipeline - Exploratory Data Analysis\n",
    "\n",
    "In the following notebooks, we will go through the implementation of each of the steps in the Machine Learning Pipeline. \n",
    "\n",
    "We will discuss:\n",
    "\n",
    "1. **Data Analysis**\n",
    "2. Feature Engineering\n",
    "3. Feature Selection\n",
    "4. Model Training\n",
    "5. Obtaining Predictions / Scoring\n",
    "\n",
    "\n",
    "We will use the house price dataset available on [Kaggle.com](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data).\n",
    "\n",
    "===================================================================================================\n",
    "\n",
    "## Predicting Sale Price of Houses\n",
    "\n",
    "The aim of the project is to build a machine learning model to predict the sale price of homes based on different explanatory variables describing aspects of residential houses.\n",
    "\n",
    "\n",
    "\n",
    "### Why is this important? \n",
    "\n",
    "Predicting house prices is useful to identify fruitful investments or to determine whether the price advertised for a house is over or under-estimated.\n",
    "\n",
    "\n",
    "### What is the objective of the machine learning model?\n",
    "\n",
    "We aim to minimise the difference between the real price and the price estimated by our model. We will evaluate model performance with the:\n",
    "\n",
    "1. mean squared error (mse)\n",
    "2. root squared of the mean squared error (rmse)\n",
    "3. r-squared (r2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb53db35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.4.3-cp39-cp39-win_amd64.whl (10.6 MB)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2022.1-py2.py3-none-any.whl (503 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\taimoor razi\\.conda\\envs\\house_prices\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Collecting numpy>=1.18.5\n",
      "  Downloading numpy-1.23.0-cp39-cp39-win_amd64.whl (14.7 MB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\taimoor razi\\.conda\\envs\\house_prices\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, numpy, pandas\n",
      "Successfully installed numpy-1.23.0 pandas-1.4.3 pytz-2022.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "scikit-learn 1.1.1 requires joblib>=1.0.0, which is not installed.\n",
      "scikit-learn 1.1.1 requires scipy>=1.3.2, which is not installed.\n",
      "scikit-learn 1.1.1 requires threadpoolctl>=2.0.0, which is not installed.\n",
      "imbalanced-learn 0.9.1 requires joblib>=1.0.0, which is not installed.\n",
      "imbalanced-learn 0.9.1 requires scipy>=1.3.2, which is not installed.\n",
      "imbalanced-learn 0.9.1 requires threadpoolctl>=2.0.0, which is not installed.\n",
      "feature-engine 1.4.1 requires scipy>=1.4.1, which is not installed.\n",
      "feature-engine 1.4.1 requires statsmodels>=0.11.1, which is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.4.3-cp39-cp39-win_amd64.whl (10.6 MB)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2022.1-py2.py3-none-any.whl (503 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\taimoor razi\\.conda\\envs\\house_prices\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Collecting numpy>=1.18.5\n",
      "  Downloading numpy-1.23.0-cp39-cp39-win_amd64.whl (14.7 MB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\taimoor razi\\.conda\\envs\\house_prices\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, numpy, pandas\n",
      "Successfully installed numpy-1.23.0 pandas-1.4.3 pytz-2022.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "scikit-learn 1.1.1 requires joblib>=1.0.0, which is not installed.\n",
      "scikit-learn 1.1.1 requires scipy>=1.3.2, which is not installed.\n",
      "scikit-learn 1.1.1 requires threadpoolctl>=2.0.0, which is not installed.\n",
      "imbalanced-learn 0.9.1 requires joblib>=1.0.0, which is not installed.\n",
      "imbalanced-learn 0.9.1 requires scipy>=1.3.2, which is not installed.\n",
      "imbalanced-learn 0.9.1 requires threadpoolctl>=2.0.0, which is not installed.\n",
      "feature-engine 1.4.1 requires scipy>=1.4.1, which is not installed.\n",
      "feature-engine 1.4.1 requires statsmodels>=0.11.1, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a490d90",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bb51227",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# to handle datasets\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# for plotting\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# to handle datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# for the yeo-johnson transformation\n",
    "import scipy.stats as stats\n",
    "\n",
    "# to display all the columns of the dataframe in the notebook\n",
    "pd.pandas.set_option('display.max_columns', None)\n",
    "\n",
    "# to avoid future warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27588a4a",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b778bee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "# rows and columns of the data\n",
    "print(df.shape)\n",
    "\n",
    "# visualise the dataset\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fafb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop id, it is just a number given to identify each house\n",
    "df.drop('Id', axis=1, inplace=True)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcb91f7",
   "metadata": {},
   "source": [
    "The house price dataset contains 1460 rows, that is, houses, and 80 columns, i.e., variables. \n",
    "\n",
    "79 are predictive variables and 1 is the target variable: SalePrice\n",
    "\n",
    "## Analysis\n",
    "\n",
    "**We will analyse the following:**\n",
    "\n",
    "1. The target variable\n",
    "2. Variable types (categorical and numerical)\n",
    "3. Missing data\n",
    "4. Numerical variables\n",
    "    - Discrete\n",
    "    - Continuous\n",
    "    - Distributions\n",
    "    - Transformations\n",
    "\n",
    "5. Categorical variables\n",
    "    - Cardinality\n",
    "    - Rare Labels\n",
    "    - Special mappings\n",
    "    \n",
    "6. Additional Reading Resources\n",
    "\n",
    "## Target\n",
    "\n",
    "Let's begin by exploring the target distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e693df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"SalePrice\"].hist(bins=50, density = True)\n",
    "plt.xlabel(\"Sale Price\")\n",
    "plt.ylabel(\"Number of Houses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3e1ad4",
   "metadata": {},
   "source": [
    "We can see that the target is continuous, and the distribution is skewed towards the right.\n",
    "\n",
    "We can improve the value spread with a mathematical transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ffefc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's transform the target using the logarithm\n",
    "\n",
    "np.log(df[\"SalePrice\"]).hist(bins=50, density=True)\n",
    "plt.ylabel(\"Number of houses\")\n",
    "plt.xlabel(\"Log of Sale Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5c388d",
   "metadata": {},
   "source": [
    "Now the distribution looks more Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d0b33d",
   "metadata": {},
   "source": [
    "## Variable Types\n",
    "\n",
    "Next, let's identify the categorical and numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7ec2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8316127c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's identify the categorical variables\n",
    "# we will capture those of type *object*\n",
    "\n",
    "cat_vars = [var for var in df.columns if df[var].dtype == 'O']\n",
    "\n",
    "# MSSubClass is also categorical by definition, despite its numeric values\n",
    "# (The definitions of the variables can be seen in the data_description.txt)\n",
    "\n",
    "# lets add MSSubClass to the list of categorical variables\n",
    "cat_vars = cat_vars + ['MSSubClass']\n",
    "\n",
    "# number of categorical variables\n",
    "len(cat_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7ba8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast all variables as categorical\n",
    "df[cat_vars] = df[cat_vars].astype('O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0dc2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's identify the numerical variables\n",
    "\n",
    "num_vars = [\n",
    "    var for var in df.columns if var not in cat_vars and var != \"SalePrice\"\n",
    "]\n",
    "\n",
    "# number of numerical variables\n",
    "len(num_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f95448",
   "metadata": {},
   "source": [
    "# Missing values\n",
    "\n",
    "Let's go ahead and find out which variables of the dataset contain missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d3da16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8423aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0de645",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns[df.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09c2eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of the variables that contain missing values\n",
    "vars_with_na = [var for var in df.columns if df[var].isnull().sum() > 0]\n",
    "\n",
    "# determine percentage of missing values (expressed as decimals)\n",
    "# and display the result ordered by % of missin data\n",
    "\n",
    "df[vars_with_na].isnull().mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af637cdc",
   "metadata": {},
   "source": [
    "Our dataset contains a few variables with a big proportion of missing values (4 variables at the top). And some other variables with a small percentage of missing observations.\n",
    "\n",
    "This means that to train a machine learning model with this data set, we need to impute the missing data in these variables.\n",
    "\n",
    "We can also visualize the percentage of missing values in the variables as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e71bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "\n",
    "df[vars_with_na].isnull().mean().sort_values(\n",
    "    ascending=False).plot.bar(figsize=(10, 4))\n",
    "plt.ylabel('Percentage of missing data')\n",
    "plt.axhline(y=0.90, color='r', linestyle='-')\n",
    "plt.axhline(y=0.80, color='g', linestyle='-')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bd843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can determine which variables, from those with missing data,\n",
    "# are numerical and which are categorical\n",
    "\n",
    "cat_na = [var for var in cat_vars if var in vars_with_na]\n",
    "num_na = [var for var in num_vars if var in vars_with_na]\n",
    "\n",
    "print('Number of categorical variables with na: ', len(cat_na))\n",
    "print('Number of numerical variables with na: ', len(num_na))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5144c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8bfde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_na"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ce84f0",
   "metadata": {},
   "source": [
    "## Relationship between missing data and Sale Price\n",
    "\n",
    "Let's evaluate the price of the house in those observations where the information is missing. We will do this for each variable that shows missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e009cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_na_value(df, var):\n",
    "\n",
    "    # copy of the dataframe, so that we do not override the original data\n",
    "    # see the link for more details about pandas.copy()\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.copy.html\n",
    "    df = df.copy()\n",
    "\n",
    "    # let's make an interim variable that indicates 1 if the\n",
    "    # observation was missing or 0 otherwise\n",
    "    df[var] = np.where(df[var].isnull(), 1, 0)\n",
    "\n",
    "    # let's compare the median SalePrice in the observations where data is missing\n",
    "    # vs the observations where data is available\n",
    "\n",
    "    # determine the median price in the groups 1 and 0,\n",
    "    # and the standard deviation of the sale price,\n",
    "    # and we capture the results in a temporary dataset\n",
    "    tmp = df.groupby(var)['SalePrice'].agg(['mean', 'std'])\n",
    "\n",
    "    # plot into a bar graph\n",
    "    tmp.plot(kind=\"barh\", y=\"mean\", legend=False,\n",
    "             xerr=\"std\", title=\"Sale Price\", color='green')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4bebb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's run the function on each variable with missing data\n",
    "\n",
    "for var in vars_with_na:\n",
    "    analyse_na_value(df, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ac75dd",
   "metadata": {},
   "source": [
    "In some variables, the average Sale Price in houses where the information is missing, differs from the average Sale Price in houses where information exists. This suggests that data being missing could be a good predictor of Sale Price."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339f87c4",
   "metadata": {},
   "source": [
    "# Numerical variables\n",
    "\n",
    "Let's go ahead and find out what numerical variables we have in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d291db7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of numerical variables: ', len(num_vars))\n",
    "\n",
    "# visualise the numerical variables\n",
    "df[num_vars].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5ca2bb",
   "metadata": {},
   "source": [
    "## Temporal variables\n",
    "\n",
    "We have 4 year variables in the dataset:\n",
    "\n",
    "- YearBuilt: year in which the house was built\n",
    "- YearRemodAdd: year in which the house was remodeled\n",
    "- GarageYrBlt: year in which a garage was built\n",
    "- YrSold: year in which the house was sold\n",
    "\n",
    "We generally don't use date variables in their raw format. Instead, we extract information from them. For example, we can capture the difference in years between the year the house was built and the year the house was sold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969ef607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of variables that contain year information\n",
    "\n",
    "year_vars = [var for var in num_vars if \"Yr\" in var or \"Year\" in var]\n",
    "\n",
    "year_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdb2ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's explore the values of these temporal variables\n",
    "\n",
    "for var in year_vars:\n",
    "    print(var, df[var].unique())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91880d6d",
   "metadata": {},
   "source": [
    "As expected, the values are years.\n",
    "\n",
    "We can explore the evolution of the sale price with the years in which the house was sold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7766bd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot median sale price vs year in which it was sold\n",
    "\n",
    "df.groupby(\"YrSold\")[\"SalePrice\"].median().plot()\n",
    "plt.ylabel(\"Median House Price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6796532c",
   "metadata": {},
   "source": [
    "There has been a drop in the value of the houses. That is unusual, in real life, house prices typically go up as years go by.\n",
    "\n",
    "Let's explore a bit further. \n",
    "\n",
    "Let's plot the price of sale vs year in which it was built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed695da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot median sale price vs year in which it was built\n",
    "\n",
    "df.groupby(\"YearBuilt\")[\"SalePrice\"].median().plot()\n",
    "plt.ylabel(\"Median House Price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c0a4fb",
   "metadata": {},
   "source": [
    "We can see that newly built / younger houses tend to be more expensive.\n",
    "\n",
    "Could it be that lately older houses were sold? Let's have a look at that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb89d7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_year_vars(df, var):\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # capture difference between a year variable and year\n",
    "    # in which the house was sold\n",
    "    df[var] = df['YrSold'] - df[var]\n",
    "    \n",
    "    df.groupby('YrSold')[var].median().plot()\n",
    "    plt.ylabel('Time from ' + var)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "for var in year_vars:\n",
    "    if var !='YrSold':\n",
    "        analyse_year_vars(df, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1ddf2a",
   "metadata": {},
   "source": [
    "From the plots, we see that towards 2010, the houses sold had older garages, and had not been remodelled recently, that might explain why we see cheaper sales prices in recent years, at least in this dataset.\n",
    "\n",
    "We can now plot instead the time since last remodelled, or time since built, and sale price, to see if there is a relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db8fb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_year_vars(df, var):\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # capture difference between a year variable and year\n",
    "    # in which the house was sold\n",
    "    df[var] = df['YrSold'] - df[var]\n",
    "    \n",
    "    plt.scatter(df[var], df['SalePrice'])\n",
    "    plt.ylabel('SalePrice')\n",
    "    plt.xlabel(var)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "for var in year_vars:\n",
    "    if var !='YrSold':\n",
    "        analyse_year_vars(df, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a3813a",
   "metadata": {},
   "source": [
    "We see that there is a tendency to a decrease in price, with older houses. In other words, the longer the time between the house was built or remodeled and sale date, the lower the sale Price. \n",
    "\n",
    "Which makes sense, cause this means that the house will have an older look, and potentially needs repairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6695574",
   "metadata": {},
   "source": [
    "## Discrete variables\n",
    "\n",
    "Let's go ahead and find which variables are discrete, i.e., show a finite number of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161eed3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[num_vars].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8863a660",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  let's male a list of discrete variables\n",
    "discrete_vars = [var for var in num_vars if len(\n",
    "    df[var].unique()) < 20 and var not in year_vars]\n",
    "\n",
    "\n",
    "print('Number of discrete variables: ', len(discrete_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f6105b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's visualise the discrete variables\n",
    "\n",
    "df[discrete_vars].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22f942d",
   "metadata": {},
   "source": [
    "These discrete variables tend to be qualifications (Qual) or grading scales (Cond), or refer to the number of rooms, or units (FullBath, GarageCars), or indicate the area of the room (KitchenAbvGr).\n",
    "\n",
    "We expect higher prices, with bigger numbers.\n",
    "\n",
    "Let's go ahead and analyse their contribution to the house price.\n",
    "\n",
    "MoSold is the month in which the house was sold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c650c573",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in discrete_vars:\n",
    "    # make boxplot with Catplot\n",
    "    sns.catplot(x=var, y='SalePrice', data=df, kind=\"box\", height=4, aspect=1.5)\n",
    "    # add data points to boxplot with stripplot\n",
    "    sns.stripplot(x=var, y='SalePrice', data=df, jitter=0.1, alpha=0.3, color='k')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86699a10",
   "metadata": {},
   "source": [
    "For most discrete numerical variables, we see an increase in the sale price, with the quality, or overall condition, or  number of rooms, or surface.\n",
    "\n",
    "For some variables, we don't see this tendency. Most likely that variable is not a good predictor of sale price."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff046b8",
   "metadata": {},
   "source": [
    "## Continuous variables\n",
    "\n",
    "Let's go ahead and find the distribution of the continuous variables. We will consider continuous variables to all those that are not temporal or discrete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2490a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make list of continuous variables\n",
    "cont_vars = [\n",
    "    var for var in num_vars if var not in discrete_vars+year_vars]\n",
    "\n",
    "print('Number of continuous variables: ', len(cont_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d909ebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's visualise the continuous variables\n",
    "\n",
    "df[cont_vars].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d467507b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets plot histograms for all continuous variables\n",
    "\n",
    "df[cont_vars].hist(bins=30, figsize=(15,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebdb15b",
   "metadata": {},
   "source": [
    "The variables are not normally distributed. And there are a particular few that are extremely skewed like 3SsnPorch, ScreenPorch and MiscVal.\n",
    "\n",
    "Sometimes, transforming the variables to improve the value spread, improves the model performance. But it is unlikely that a transformation will help change the distribution of the super skewed variables dramatically.\n",
    "\n",
    "We can apply a Yeo-Johnson transformation to variables like LotFrontage, LotArea, BsmUnfSF, and a binary transformation to variables like 3SsnPorch, ScreenPorch and MiscVal.\n",
    "\n",
    "Let's go ahead and do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06770d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first make a list with the super skewed variables\n",
    "# for later\n",
    "\n",
    "skewed = [\n",
    "    'BsmtFinSF2', 'LowQualFinSF', 'EnclosedPorch',\n",
    "    '3SsnPorch', 'ScreenPorch', 'MiscVal'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb404cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture the remaining continuous variables\n",
    "\n",
    "cont_vars = [\n",
    "    'LotFrontage',\n",
    "    'LotArea',\n",
    "    'MasVnrArea',\n",
    "    'BsmtFinSF1',\n",
    "    'BsmtUnfSF',\n",
    "    'TotalBsmtSF',\n",
    "    '1stFlrSF',\n",
    "    '2ndFlrSF',\n",
    "    'GrLivArea',\n",
    "    'GarageArea',\n",
    "    'WoodDeckSF',\n",
    "    'OpenPorchSF',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c073da18",
   "metadata": {},
   "source": [
    "### Yeo-Johnson transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f631f1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go ahead and analyse the distributions of the variables\n",
    "# after applying a yeo-johnson transformation\n",
    "\n",
    "# temporary copy of the data\n",
    "tmp = df.copy()\n",
    "\n",
    "for var in cont_vars:\n",
    "\n",
    "    # transform the variable - yeo-johsnon\n",
    "    tmp[var], param = stats.yeojohnson(df[var])\n",
    "\n",
    "    \n",
    "# plot the histograms of the transformed variables\n",
    "tmp[cont_vars].hist(bins=30, figsize=(15,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2c7df8",
   "metadata": {},
   "source": [
    "For LotFrontage and MasVnrArea the transformation did not do an amazing job. \n",
    "\n",
    "For the others, the values seem to be spread more evenly in the range.\n",
    "\n",
    "Whether this helps improve the predictive power, remains to be seen. To determine if this is the case, we should train a model with the original values and one with the transformed values, and determine model performance, and feature importance. But that escapes the scope of this course.\n",
    "\n",
    "Here, we will do a quick visual exploration here instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c983786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot the original or transformed variables\n",
    "# vs sale price, and see if there is a relationship\n",
    "\n",
    "for var in cont_vars:\n",
    "    \n",
    "    plt.figure(figsize=(12,4))\n",
    "    \n",
    "    # plot the original variable vs sale price    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(df[var], np.log(df['SalePrice']))\n",
    "    plt.ylabel('Sale Price')\n",
    "    plt.xlabel('Original ' + var)\n",
    "\n",
    "    # plot transformed variable vs sale price\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(tmp[var], np.log(tmp['SalePrice']))\n",
    "    plt.ylabel('Sale Price')\n",
    "    plt.xlabel('Transformed ' + var)\n",
    "                \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f03807f",
   "metadata": {},
   "source": [
    "By eye, the transformations seems to improve the relationship only for LotArea.\n",
    "\n",
    "Let's try a different transformation now. Most variables contain the value 0, and thus we can't apply the logarithmic transformation, but we can certainly do that for the following variables:\n",
    "\n",
    " [\"LotFrontage\", \"1stFlrSF\", \"GrLivArea\"]\n",
    " \n",
    " So let's do that and see if that changes the variable distribution and its relationship with the target.\n",
    " \n",
    " ### Logarithmic transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bb17f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go ahead and analyse the distributions of these variables\n",
    "# after applying a logarithmic transformation\n",
    "\n",
    "tmp = df.copy()\n",
    "\n",
    "for var in [\"LotFrontage\", \"1stFlrSF\", \"GrLivArea\"]:\n",
    "\n",
    "    # transform the variable with logarithm\n",
    "    tmp[var] = np.log(df[var])\n",
    "    \n",
    "tmp[[\"LotFrontage\", \"1stFlrSF\", \"GrLivArea\"]].hist(bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdf3361",
   "metadata": {},
   "source": [
    "The distribution of the variables are now more \"Gaussian\" looking.\n",
    "\n",
    "Let's go ahead and evaluate their relationship with the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0424fdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot the original or transformed variables\n",
    "# vs sale price, and see if there is a relationship\n",
    "\n",
    "for var in [\"LotFrontage\", \"1stFlrSF\", \"GrLivArea\"]:\n",
    "    \n",
    "    plt.figure(figsize=(12,4))\n",
    "    \n",
    "    # plot the original variable vs sale price    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(df[var], np.log(df['SalePrice']))\n",
    "    plt.ylabel('Sale Price')\n",
    "    plt.xlabel('Original ' + var)\n",
    "\n",
    "    # plot transformed variable vs sale price\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(tmp[var], np.log(tmp['SalePrice']))\n",
    "    plt.ylabel('Sale Price')\n",
    "    plt.xlabel('Transformed ' + var)\n",
    "                \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68f61e8",
   "metadata": {},
   "source": [
    "The transformed variables have a better spread of the values, which may in turn, help make better predictions.\n",
    "\n",
    "## Skewed variables\n",
    "\n",
    "Let's transform them into binary variables and see how predictive they are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b22aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in skewed:\n",
    "    \n",
    "    tmp = df.copy()\n",
    "    \n",
    "    # map the variable values into 0 and 1\n",
    "    tmp[var] = np.where(df[var]==0, 0, 1)\n",
    "    \n",
    "    # determine mean sale price in the mapped values\n",
    "    tmp = tmp.groupby(var)[\"SalePrice\"].agg([\"mean\", \"std\"])\n",
    "\n",
    "    # plot into a bar graph\n",
    "    tmp.plot(kind=\"barh\", y=\"mean\", legend=False,\n",
    "             xerr=\"std\", title=\"Sale Price\", color='green')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510de687",
   "metadata": {},
   "source": [
    "There seem to be a difference in Sale Price in the mapped values, but the confidence intervals overlap, so most likely this is not significant or predictive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c193076",
   "metadata": {},
   "source": [
    "# Categorical variables\n",
    "\n",
    "Let's go ahead and analyse the categorical variables present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f480fc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of categorical variables: \", len(cat_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b858329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's visualise the values of the categorical variables\n",
    "df[cat_vars].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c7349d",
   "metadata": {},
   "source": [
    "## Number of labels: cardinality\n",
    "\n",
    "Let's evaluate how many different categories are present in each of the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acda9d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we count unique categories with pandas unique() \n",
    "# and then plot them in descending order\n",
    "\n",
    "df[cat_vars].nunique().sort_values(ascending=False).plot.bar(figsize=(12,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07275f1a",
   "metadata": {},
   "source": [
    "All the categorical variables show low cardinality, this means that they have only few different labels. That is good as we won't need to tackle cardinality during feature engineering.\n",
    "\n",
    "## Quality variables\n",
    "\n",
    "There are a number of variables that refer to the quality of some aspect of the house, for example the garage, or the fence, or the kitchen. I will replace these categories by numbers increasing with the quality of the place or room.\n",
    "\n",
    "The mappings can be obtained from the Kaggle Website. One example:\n",
    "\n",
    "- Ex = Excellent\n",
    "- Gd = Good\n",
    "- TA = Average/Typical\n",
    "- Fa =\tFair\n",
    "- Po = Poor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495abda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-map strings to numbers, which determine quality\n",
    "\n",
    "qual_mappings = {\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5, \"Missing\": 0, \"NA\": 0}\n",
    "\n",
    "qual_vars = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond',\n",
    "             'HeatingQC', 'KitchenQual', 'FireplaceQu',\n",
    "             'GarageQual', 'GarageCond',\n",
    "            ]\n",
    "\n",
    "for var in qual_vars:\n",
    "    df[var] = df[var].map(qual_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c861cc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "exposure_mappings = {\"No\": 1, \"Mn\": 2, \"Av\": 3, \"Gd\": 4, \"Missing\": 0, \"NA\": 0}\n",
    "\n",
    "var = \"BsmtExposure\"\n",
    "\n",
    "df[var] = df[var].map(exposure_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c175d8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "finish_mappings = {\"Missing\": 0, \"NA\": 0, \"Unf\": 1, \"LwQ\": 2, \"Rec\": 3, \"BLQ\": 4, \"ALQ\": 5, \"GLQ\": 6}\n",
    "\n",
    "finish_vars = [\"BsmtFinType1\", \"BsmtFinType2\"]\n",
    "\n",
    "for var in finish_vars:\n",
    "    df[var] = df[var].map(finish_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34af9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "garage_mappings = {\"Missing\": 0, \"NA\": 0, \"Unf\": 1, \"RFn\": 2, \"Fin\": 3}\n",
    "\n",
    "var = \"GarageFinish\"\n",
    "\n",
    "df[var] = df[var].map(garage_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcebf180",
   "metadata": {},
   "outputs": [],
   "source": [
    "fence_mappings = {\"Missing\": 0, \"NA\": 0, \"MnWw\": 1, \"GdWo\": 2, \"MnPrv\": 3, \"GdPrv\": 4}\n",
    "\n",
    "var = \"Fence\"\n",
    "\n",
    "df[var] = df[var].map(fence_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5177d390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture all quality variables\n",
    "\n",
    "qual_vars  = qual_vars + finish_vars + [\"BsmtExposure\",\"GarageFinish\",\"Fence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6a5c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's plot the house mean sale price based on the quality of the \n",
    "# various attributes\n",
    "\n",
    "for var in qual_vars:\n",
    "    # make boxplot with Catplot\n",
    "    sns.catplot(x=var, y=\"SalePrice\", data=df, kind=\"box\", height=4, aspect=1.5)\n",
    "    # add data points to boxplot with stripplot\n",
    "    sns.stripplot(x=var, y=\"SalePrice\", data=df, jitter=0.1, alpha=0.3, color=\"k\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19a4868",
   "metadata": {},
   "source": [
    "For most attributes, the increase in the house price with the value of the variable, is quite clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8d4e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture the remaining categorical variables\n",
    "# (those that we did not re-map)\n",
    "\n",
    "cat_others = [\n",
    "    var for var in cat_vars if var not in qual_vars\n",
    "]\n",
    "\n",
    "len(cat_others)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f82b8a",
   "metadata": {},
   "source": [
    "## Rare labels:\n",
    "\n",
    "Let's go ahead and investigate now if there are labels that are present only in a small number of houses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2bd4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_rare_labels(df, var, rare_perc):\n",
    "    df = df.copy()\n",
    "\n",
    "    # determine the % of observations per category\n",
    "    tmp = df.groupby(var)['SalePrice'].count() / len(df)\n",
    "\n",
    "    # return categories that are rare\n",
    "    return tmp[tmp < rare_perc]\n",
    "\n",
    "# print categories that are present in less than\n",
    "# 1 % of the observations\n",
    "\n",
    "for var in cat_others:\n",
    "    print(analyse_rare_labels(df, var, 0.01))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3f4ae2",
   "metadata": {},
   "source": [
    "Some of the categorical variables show multiple labels that are present in less than 1% of the houses. \n",
    "\n",
    "Labels that are under-represented in the dataset tend to cause over-fitting of machine learning models. \n",
    "\n",
    "That is why we want to remove them.\n",
    "\n",
    "Finally, we want to explore the relationship between the categories of the different variables and the house sale price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad1642a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in cat_others:\n",
    "    # make boxplot with Catplot\n",
    "    sns.catplot(x=var, y='SalePrice', data=df, kind=\"box\", height=4, aspect=1.5)\n",
    "    # add data points to boxplot with stripplot\n",
    "    sns.stripplot(x=var, y='SalePrice', data=df, jitter=0.1, alpha=0.3, color='k')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3106856",
   "metadata": {},
   "source": [
    "Clearly, the categories give information on the SalePrice, as different categories show different median sale prices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac98ec3",
   "metadata": {},
   "source": [
    "**Disclaimer:**\n",
    "\n",
    "There is certainly more that can be done to understand the nature of this data and the relationship of these variables with the target, SalePrice. And also about the distribution of the variables themselves.\n",
    "\n",
    "However, I hope that through this notebook I gave you a flavour of what data analysis looks like."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
